{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://supportvectors.ai/logo-poster-transparent.png\" width=\"400px\" style=\"opacity:0.7\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!-- Many of the styles here are inspired by: \n",
       "    https://towardsdatascience.com/10-practical-tips-you-need-to-know-to-personalize-jupyter-notebook-fbd202777e20 \n",
       "\n",
       "\n",
       "    On the author's local machine, these exist in the custom.css file. However, in order to keep uniform look and feel, \n",
       "    and at the request of participants, I have added it to this common import-file here.\n",
       "\n",
       "    -->\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Lora:400,700|Montserrat:300\" rel=\"stylesheet\">\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Crimson+Pro&family=Literata&display=swap\" rel=\"stylesheet\">\n",
       "<style>\n",
       "\n",
       "\n",
       "#ipython_notebook::before{\n",
       " content:\"Neural Architectures\";\n",
       "        color: white;\n",
       "        font-weight: bold;\n",
       "        text-transform: uppercase;\n",
       "        font-family: 'Lora',serif;\n",
       "        font-size:16pt;\n",
       "        margin-bottom:15px;\n",
       "        margin-top:15px;\n",
       "\n",
       "}\n",
       "body > #header {\n",
       "    #background: #D15555;\n",
       "    background: linear-gradient(to bottom, indianred 0%, #fff 100%);\n",
       "    opacity: 0.8;\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       ".navbar-default .navbar-nav > li > a, #kernel_indicator {\n",
       "    color: white;\n",
       "    transition: all 0.25s;\n",
       "    font-size:10pt;\n",
       "    font-family: sans-serif;\n",
       "    font-weight:normal;\n",
       "}\n",
       ".navbar-default {\n",
       "    padding-left:100px;\n",
       "    background: none;\n",
       "    border: none;\n",
       "}\n",
       "\n",
       "\n",
       "body > menubar-container {\n",
       "    background-color: wheat;\n",
       "}\n",
       "#ipython_notebook img{                                                                                        \n",
       "    display:block; \n",
       "\n",
       "    background: url(\"https://www.supportvectors.com/wp-content/uploads/2016/03/logo-poster-smaller.png\") no-repeat;\n",
       "    background-size: contain;\n",
       "\n",
       "    padding-left: 600px;\n",
       "    padding-right: 100px;\n",
       "\n",
       "    -moz-box-sizing: border-box;\n",
       "    box-sizing: border-box;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "body {\n",
       " #font-family:  'Literata', serif;\n",
       "    font-family:'Lora', san-serif;\n",
       "    text-align: justify;\n",
       "    font-weight: 400;\n",
       "    font-size: 12pt;\n",
       "}\n",
       "\n",
       "iframe{\n",
       "    width:100%;\n",
       "    min-height:600px;\n",
       "}\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "# font-family: 'Montserrat', sans-serif;\n",
       " font-family:'Lora', serif;\n",
       " font-weight: 200;\n",
       " text-transform: uppercase;\n",
       " color: #EC7063 ;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "    color: #000080;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       "#notebook_name {\n",
       "    font-weight: 600;\n",
       "    font-size:20pt;\n",
       "    text-variant:uppercase;\n",
       "    color: wheat; \n",
       "    margin-right:20px;\n",
       "    margin-left:-500px;\n",
       "}\n",
       "#notebook_name:hover {\n",
       "background-color: salmon;\n",
       "}\n",
       "\n",
       "\n",
       ".dataframe { /* dataframe atau table */\n",
       "    background: white;\n",
       "    box-shadow: 0px 1px 2px #bbb;\n",
       "}\n",
       ".dataframe thead th, .dataframe tbody td {\n",
       "    text-align: center;\n",
       "    padding: 1em;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       ".output {\n",
       "    align-items: center;\n",
       "}\n",
       "\n",
       "div.cell {\n",
       "    transition: all 0.25s;\n",
       "    border: none;\n",
       "    position: relative;\n",
       "    top: 0;\n",
       "}\n",
       "div.cell.selected, div.cell.selected.jupyter-soft-selected {\n",
       "    border: none;\n",
       "    background: transparent;\n",
       "    box-shadow: 0 6px 18px #aaa;\n",
       "    z-index: 10;\n",
       "    top: -10px;\n",
       "}\n",
       ".CodeMirror pre, .CodeMirror-dialog, .CodeMirror-dialog .CodeMirror-search-field, .terminal-app .terminal {\n",
       "    font-family: 'Hack' , serif; \n",
       "    font-weight: 500;\n",
       "    font-size: 14pt;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "</style>    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "<div style=\"color:#aaa;font-size:8pt\">\n",
       "<hr/>\n",
       "&copy; SupportVectors. All rights reserved. <blockquote>This notebook is the intellectual property of SupportVectors, and part of its training material. \n",
       "Only the participants in SupportVectors workshops are allowed to study the notebooks for educational purposes currently, but is prohibited from copying or using it for any other purposes without written permission.\n",
       "\n",
       "<b> These notebooks are chapters and sections from Asif Qamar's textbook that he is writing on Data Science. So we request you to not circulate the material to others.</b>\n",
       " </blockquote>\n",
       " <hr/>\n",
       "</div>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: Building Enterprise AI Systems\n",
    "\n",
    "## Overview\n",
    "- The bootcamp emphasizes transitioning from **proof-of-concept AI** to **fully operational enterprise systems**.\n",
    "- Focus on **scaling AI** beyond experimentation into **real-world production environments**.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Components of AI Systems\n",
    "\n",
    "<img src='../images/ai_system.png' width=1000px>\n",
    "\n",
    "\n",
    "### 1. Data Infrastructure\n",
    "- **Data is foundational** ‚Äî often referred to as the \"food\" for AI.\n",
    "- **Data-related tasks consume ~80%** of AI development efforts.\n",
    "- Despite the \"big data\" era, **data scarcity and quality remain major challenges**.\n",
    "- Emphasis on **data engineering** and **feature engineering** to ensure effective model training.\n",
    "\n",
    "<img src='../images/AI_infra.png' width=1000px>\n",
    "\n",
    "\n",
    "### 2. Technical Building Blocks\n",
    "- **Models & Algorithms** form the mathematical backbone of AI.\n",
    "- **Machine Learning platforms** provide the environment for model development and deployment.\n",
    "- **Specialized AI hardware** (e.g., NVIDIA Blackwell, Huawei chips) is essential for performance and scalability.\n",
    "\n",
    "### 3. Operational Aspects (MLOps)\n",
    "- **MLOps** differs significantly from traditional DevOps/SysOps.\n",
    "- Key responsibilities include:\n",
    "  - Implementing **security guardrails**\n",
    "  - Ensuring **system observability**\n",
    "  - Conducting **performance monitoring**\n",
    "  - Managing **scalability** and **latency**\n",
    "\n",
    "\n",
    "### 4. Critical Considerations\n",
    "- **AI Security**: Enterprise AI systems are vulnerable and must be protected against adversarial threats.\n",
    "- **Interpretability**: Understanding how AI systems make decisions is crucial.\n",
    "- **Iterative Development**: Includes building, training, fine-tuning, and benchmarking AI models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: From Keyword Search to Semantic Search with Transformers and Embeddings\n",
    "## üß≠ 1. Introduction\n",
    "\n",
    "Search is fundamental to how we access knowledge ‚Äî from finding files in a company knowledge base to retrieving answers from the web. Traditionally, search was based on keywords. But language is nuanced, and keyword-based systems often miss the mark.\n",
    "\n",
    "This notebook walks through the evolution from traditional **keyword search** to modern **semantic search** powered by **transformers** and **contextual embeddings**.\n",
    "\n",
    "## üìö 2. Traditional Search and the Inverted Index\n",
    "\n",
    "### üîç What is an Inverted Index?\n",
    "\n",
    "An inverted index is a mapping from **keywords** to the **documents** that contain them.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Doc1: \"The cow jumped over the moon.\"\n",
    "Doc2: \"To go to the moon you need a big rocket.\"\n",
    "```\n",
    "\n",
    "**Inverted Index:**\n",
    "```\n",
    "cow     ‚Üí [Doc1]\n",
    "moon    ‚Üí [Doc1, Doc2]\n",
    "rocket  ‚Üí [Doc2]\n",
    "```\n",
    "\n",
    "### ‚ùå Limitations\n",
    "\n",
    "- Ignores context (e.g., \"bank\" can mean money bank or river bank)\n",
    "- Hard to rank relevance effectively\n",
    "- Can't handle paraphrased queries\n",
    "- Keyword match ‚â† semantic match\n",
    "\n",
    "## üî¢ 3. Vectors and Embeddings\n",
    "\n",
    "### üßÆ What are Vectors?\n",
    "\n",
    "- **Scalar:** Single number (0D)\n",
    "- **Vector:** Array of numbers (1D)\n",
    "- **Matrix:** 2D array\n",
    "- **Tensor:** Generalization to n-dimensions\n",
    "\n",
    "### üí° Why Vectors?\n",
    "\n",
    "All machine learning models (especially neural networks) operate on numbers.\n",
    "\n",
    "> **\"Vector in ‚Üí Vector out\"**  \n",
    "> All text, images, audio must be transformed into vectors before models can use them.\n",
    "\n",
    "### üì¶ Word Embeddings\n",
    "\n",
    "Early NLP models used static word embeddings like **Word2Vec** or **GloVe**.\n",
    "```\n",
    "Embedding(\"cow\") ‚Üí [0.23, -0.11, ..., 0.56]\n",
    "Embedding(\"moon\") ‚Üí [-0.45, 0.89, ..., -0.12]\n",
    "```\n",
    "These embeddings are **static** ‚Äî the same word always has the same vector.\n",
    "\n",
    "## üß† 4. The Problem of Context\n",
    "\n",
    "Take the word **\"bank\"**:\n",
    "\n",
    "- \"He went to the bank to deposit money.\"\n",
    "- \"She sat by the river bank.\"\n",
    "- \"I'm banking on you to help.\"\n",
    "\n",
    "Using a single vector for **\"bank\"** is misleading. The meaning comes from **context**.\n",
    "\n",
    "## üéØ 5. Attention and Transformers\n",
    "\n",
    "### üåü Intuition for Attention\n",
    "\n",
    "> Attention allows the model to **focus** on relevant parts of input based on the context.\n",
    "\n",
    "**Analogy:**  \n",
    "You're hiking, enjoying the stars. Suddenly you hear a growl in the bushes ‚Äî your attention instantly shifts. Your sensory input hasn't changed, but **your context** has.\n",
    "\n",
    "### üìê Transformer = Attention is All You Need\n",
    "\n",
    "Introduced in 2017, the Transformer architecture:\n",
    "- Uses self-attention to relate words in a sentence\n",
    "- Allows **contextual interpretation** of each word\n",
    "- Foundation of today's **Large Language Models (LLMs)**\n",
    "\n",
    "## üß© 6. Contextual Word Embeddings\n",
    "\n",
    "Transformer models generate **contextual embeddings** ‚Äî where the meaning of each word is determined by the **context** it appears in.\n",
    "\n",
    "Unlike static embeddings (e.g., Word2Vec), which assign a single vector to each word regardless of usage, contextual embeddings differ based on surrounding words.\n",
    "\n",
    "**Examples:**\n",
    "```\n",
    "\"The river overflowed the bank.\"  ‚Üí  \"bank\" ‚âà shoreline\n",
    "\"He deposited cash in the bank.\"  ‚Üí  \"bank\" ‚âà financial institution\n",
    "```\n",
    "\n",
    "This is made possible by the **attention mechanism**, which helps each word understand its role based on the full sentence context.\n",
    "\n",
    "> üîÅ Every word asks: \"Who am I?\"  \n",
    "> And answers it by attending to its neighbors.\n",
    "\n",
    "## üßÆ 7. Sentence Embeddings and Representation\n",
    "\n",
    "To represent an entire sentence (not just the individual words), we need to **combine the contextual word embeddings** into one unified vector.\n",
    "\n",
    "### Two common strategies:\n",
    "\n",
    "#### 1. **Mean Pooling**\n",
    "Take the average of all word vectors in the sentence.\n",
    "\n",
    "> Sentence vector = mean(word1, word2, ..., wordN)\n",
    "\n",
    "#### 2. **[CLS] Token**\n",
    "Insert a special token `[CLS]` at the beginning of the sentence.  \n",
    "The transformer learns to use this token to capture the **entire sentence meaning**.\n",
    "\n",
    "- `[CLS]` pays attention to all words.\n",
    "- It starts with no meaning but learns it **entirely from context**.\n",
    "- Used by models like BERT for classification or semantic tasks.\n",
    "\n",
    "> Resulting vector = semantic summary of the full sentence.\n",
    "\n",
    "This **sentence embedding** becomes a powerful input for tasks like classification, clustering, or semantic search.\n",
    "\n",
    "## ‚öñÔ∏è 8. Contrastive Learning\n",
    "\n",
    "To teach a model semantic similarity:\n",
    "\n",
    "- Sentence A: \"The cow jumped over the moon\"\n",
    "- Sentence B: \"The car leaped over the puddle\"\n",
    "- Sentence C: \"NVIDIA stock jumped 10%\"\n",
    "\n",
    "A and B are semantically related. A and C are not.\n",
    "\n",
    "### üß† Training Objective\n",
    "\n",
    "**Contrastive Loss:**\n",
    "- **Minimize** distance between A and B\n",
    "- **Maximize** distance between A and C\n",
    "\n",
    "This forces the model to place similar texts **closer in vector space**.\n",
    "\n",
    "## üß≠ 9. Semantic Search\n",
    "\n",
    "Once all your documents are converted into **semantic vectors**, search becomes:\n",
    "\n",
    "### üß± Workflow\n",
    "\n",
    "1. User enters a query ‚Üí Convert it to a vector (Q)\n",
    "2. Compare Q to all document vectors (D1, D2, ..., Dn)\n",
    "3. Return the **nearest neighbors** (e.g., by cosine similarity)\n",
    "\n",
    "### üí° Pseudo-code\n",
    "```\n",
    "Q = embed(\"How do cows behave?\")\n",
    "Docs = [D1, D2, ..., Dn]\n",
    "\n",
    "For each Di:\n",
    "    similarity[i] = cosine_similarity(Q, Di)\n",
    "\n",
    "Return top-k documents sorted by similarity\n",
    "```\n",
    "\n",
    "This is **semantic search** ‚Äî retrieving documents based on **meaning**, not just keyword overlap.\n",
    "\n",
    "## üè¢ 10. Industry Adoption\n",
    "\n",
    "### Before 2018:\n",
    "- Keyword search engines (Elasticsearch, Solr)\n",
    "- Heuristics, BM25, inverted indices\n",
    "\n",
    "### After 2018:\n",
    "- Google and Bing use neural search\n",
    "- Transformer-based retrieval models\n",
    "- Enterprises now adopting vector databases (Pinecone, FAISS, Weaviate)\n",
    "\n",
    "## üîö 11. Conclusion\n",
    "\n",
    "We‚Äôve gone from:\n",
    "- Searching **words** ‚Üí Understanding **meaning**\n",
    "- Static word embeddings ‚Üí Contextualized sentence embeddings\n",
    "- Rule-based ranking ‚Üí Learned semantic spaces\n",
    "\n",
    "This shift has transformed **search**, **recommendation**, and **language understanding** in modern AI systems.\n",
    "\n",
    "## üìù Key Terms Recap\n",
    "\n",
    "- **Inverted Index** ‚Äì Maps words to documents  \n",
    "- **Embedding** ‚Äì Vector representation of a word/sentence  \n",
    "- **Attention** ‚Äì Mechanism to weigh input relevance dynamically  \n",
    "- **CLS Token** ‚Äì Captures full sentence meaning  \n",
    "- **Contrastive Loss** ‚Äì Encourages meaningful vector spacing  \n",
    "- **Semantic Search** ‚Äì Retrieves based on meaning, not keywords  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3: UV Setup + PDF Downloader Project\n",
    "\n",
    "## Environment Setup\n",
    "- uv installation\n",
    "- ai_environment\n",
    "- uv sync --upgrade\n",
    "\n",
    "## Project: PDF Downloader\n",
    " 1. Problem\n",
    "\t 1. Use prompt in **cursor** to generate the python script to automatically download all the pdfs in the given links.\n",
    " 2. Improve the code\n",
    "\t 1. Object oriented\n",
    "\t 2. Type hinting\n",
    "\t 3. Reduce the cyclometric complexity\n",
    "\t 4. Documentation\n",
    "\t 5. Logging\n",
    "\t 6. Pre-condition checks\n",
    "\t 7. Post-condition checks\n",
    "\t 8. Simplification\n",
    "\t 9. Exception nesting\n",
    "\t 10. Unit tests\n",
    "\t 11. Architecture and Sequence diagrams in MERMAID\n",
    " 3. Sort out issues with the code and documentation, maybe using claude."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
